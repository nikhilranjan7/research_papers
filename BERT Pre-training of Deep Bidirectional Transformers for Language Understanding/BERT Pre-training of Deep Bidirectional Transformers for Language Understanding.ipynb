{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BERT](https://github.com/google-research/bert) - Bidirectional Encoder Representations from Transformers. Pre-trained language models have achieved high efficiency on sentence level tasks and token level tasks.\n",
    "\n",
    "Two existing techniques are (unidirectional):\n",
    "- Feature-based\n",
    "    - ELMo: Change of architecture for different downstream tasks\n",
    "- Fine-tuning\n",
    "    - OpenAI GPT: Fine-tuning all pre-trained parameters. This is left-to-right architecture i.e. every token can get information from previous seen tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unidirectional training is not optimal because tasks like question-answering need the whole context before prediction.\n",
    "\n",
    "BERT uses two different task as pretraining:\n",
    "- MLM (Masked language model): Here some tokens from the input are randomly masked and the goal is the predict the masked words ids given the masked sentence\n",
    "- Next Sentence Prediction: Text-pair representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unsupervised Feature-based Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly discussing about Left-to-Right context for pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Unsupervised Fine-tuning Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised training after pre-training. This helps model to learn new parameters **from scratch** for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Transfer Learning from Supervised Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as previous approach but parameters are relearned instead of adding new parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps involved, pretraining and fine-tuning. In pretraining, the model is trained on large amount of unlabeled data. Next while fine-tuning, same model is loaded with pretrained parameters. Then additional output layer is added based on requirement by downstream tasks. Now the model is retrained with downstream task specific labelled dataset without freezing any layers. Figure 1 shows pre-training (same for all tasks) and fine-tuning (task specific) architectures.\n",
    "![Figure 1](resources/bert1.png \"Bert1\")\n",
    "*Figure 1: BERT Architecture*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Paper, authors have used two model sizes: (L is number of Transformer blocks, H is hidden size, A is self attention heads)\n",
    "- BERT<sub>BASE</sub> (L=12, H=768, A=12, Total Parameters=110M)\n",
    "- BERT<sub>LARGE</sub> (L=24, H=1024, A=16, Total Parameters=340M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT<sub>BASE</sub> was mainly created to compare the results with OpenAI GPT which has the same model size. The main difference here is BERT uses bidirectional context while GPT uses left context for self-attention.\n",
    "![Figure 2](resources/bert-input.png)\n",
    "*Figure 2: BERT input representation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 shows how input in constructed for BERT model. It comprises of three different embeddings - Token embeddings, Segment embeddings, Position embeddings.\n",
    "\n",
    "Token embeddings are every word/sub-word mapped to a unique number. Every sequence also starts with special classification token [CLS] in token embeddings. The final hiddent state of this token is considered as whole representation of the sequence and is used for downstream classification tasks. A sequence can contain multiple sentences for <Question, Answer> type of dataset. Another special token [SEP] is used in token-embeddings to separate multiple sequences.\n",
    "\n",
    "Segment embeddings marks which token belong to which sentence and position embeddings are used to learn about order of input. More details about this is given in this paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pre-training BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MaskedLM\n",
    "\n",
    "In this step, randomly 15% of the token from each sequence are marked as [MASK] token and then T<sub>i</sub> (shown in Figure 1) corresponding to the mask token are used in output softmax layer for binary classification (predicted true or not). As for many fine-tuning tasks, [MASK] tokens are not used, therefore authors have only used [MASK] tokens 80% of the time. In rest 20% time, incorrect tokens are substituted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
